{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING OF TEXT :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing nltk python tooklkit for NLP task\n",
    "\n",
    "import nltk\n",
    "#nltk.download()\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text=\"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store.Should I pick up some black-eyed peas as well?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization (breaking raw text into smaller indivisual unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " '!',\n",
       " 'I',\n",
       " '’',\n",
       " 'm',\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'some',\n",
       " 'vegetables',\n",
       " '(',\n",
       " 'tomatoes',\n",
       " 'and',\n",
       " 'cucumbers',\n",
       " ')',\n",
       " 'from',\n",
       " 'the',\n",
       " 'store.Should',\n",
       " 'I',\n",
       " 'pick',\n",
       " 'up',\n",
       " 'some',\n",
       " 'black-eyed',\n",
       " 'peas',\n",
       " 'as',\n",
       " 'well',\n",
       " '?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('punkt')   # if not downloaded then remove comment and run it\n",
    "\n",
    "# carefully see the output\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word=word_tokenize(Text)\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Mr.',\n",
       " 'Smith!',\n",
       " 'I’m',\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'some',\n",
       " 'vegetables',\n",
       " '(tomatoes',\n",
       " 'and',\n",
       " 'cucumbers)',\n",
       " 'from',\n",
       " 'the',\n",
       " 'store.Should',\n",
       " 'I',\n",
       " 'pick',\n",
       " 'up',\n",
       " 'some',\n",
       " 'black-eyed',\n",
       " 'peas',\n",
       " 'as',\n",
       " 'well?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how directly we can find through python\n",
    "\n",
    "word1=Text.split(' ')\n",
    "word1\n",
    "\n",
    "# compare this result with previous result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi Mr. Smith!',\n",
       " 'I’m going to buy some vegetables (tomatoes and cucumbers) from the store.Should I pick up some black-eyed peas as well?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the output and understand at generally which point it is breaking\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent=sent_tokenize(Text)\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. N-Grams Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object ngrams at 0x000001ADE0B64D68>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "word=word_tokenize(Text)\n",
    "ngram=ngrams(word,2)  # 2 is the value of n in ngrams so i have taken 2 so it is bigram\n",
    "print(type(ngram))\n",
    "ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 'Mr.'),\n",
       " ('Mr.', 'Smith'),\n",
       " ('Smith', '!'),\n",
       " ('!', 'I'),\n",
       " ('I', '’'),\n",
       " ('’', 'm'),\n",
       " ('m', 'going'),\n",
       " ('going', 'to'),\n",
       " ('to', 'buy'),\n",
       " ('buy', 'some'),\n",
       " ('some', 'vegetables'),\n",
       " ('vegetables', '('),\n",
       " ('(', 'tomatoes'),\n",
       " ('tomatoes', 'and'),\n",
       " ('and', 'cucumbers'),\n",
       " ('cucumbers', ')'),\n",
       " (')', 'from'),\n",
       " ('from', 'the'),\n",
       " ('the', 'store.Should'),\n",
       " ('store.Should', 'I'),\n",
       " ('I', 'pick'),\n",
       " ('pick', 'up'),\n",
       " ('up', 'some'),\n",
       " ('some', 'black-eyed'),\n",
       " ('black-eyed', 'peas'),\n",
       " ('peas', 'as'),\n",
       " ('as', 'well'),\n",
       " ('well', '?')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the result\n",
    "\n",
    "list(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 'Mr.', 'Smith'),\n",
       " ('Mr.', 'Smith', '!'),\n",
       " ('Smith', '!', 'I'),\n",
       " ('!', 'I', '’'),\n",
       " ('I', '’', 'm'),\n",
       " ('’', 'm', 'going'),\n",
       " ('m', 'going', 'to'),\n",
       " ('going', 'to', 'buy'),\n",
       " ('to', 'buy', 'some'),\n",
       " ('buy', 'some', 'vegetables'),\n",
       " ('some', 'vegetables', '('),\n",
       " ('vegetables', '(', 'tomatoes'),\n",
       " ('(', 'tomatoes', 'and'),\n",
       " ('tomatoes', 'and', 'cucumbers'),\n",
       " ('and', 'cucumbers', ')'),\n",
       " ('cucumbers', ')', 'from'),\n",
       " (')', 'from', 'the'),\n",
       " ('from', 'the', 'store.Should'),\n",
       " ('the', 'store.Should', 'I'),\n",
       " ('store.Should', 'I', 'pick'),\n",
       " ('I', 'pick', 'up'),\n",
       " ('pick', 'up', 'some'),\n",
       " ('up', 'some', 'black-eyed'),\n",
       " ('some', 'black-eyed', 'peas'),\n",
       " ('black-eyed', 'peas', 'as'),\n",
       " ('peas', 'as', 'well'),\n",
       " ('as', 'well', '?')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the diffrence from above one and see how it is formed\n",
    "\n",
    "word=word_tokenize(Text)\n",
    "ngram=ngrams(word,3)\n",
    "list(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Regular Expression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both Text are exactly same only way of writing(one Text is in single line while another is in two line) is different \n",
    "\n",
    "Text=\"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes\" \\\n",
    "\"and cucumbers) from the store.Should I pick up some black-eyed peas as well?\"\n",
    "\n",
    "# Text=\"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store.Should I pick up some black-eyed peas as well?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Mr. Smith! I’m going to buy some vegetables (tomatoesand cucumbers) from the store.Should I pick up some black-eyed peas as well?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegexpTokenizer(pattern='\\\\s+', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Mr.',\n",
       " 'Smith!',\n",
       " 'I’m',\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'some',\n",
       " 'vegetables',\n",
       " '(tomatoesand',\n",
       " 'cucumbers)',\n",
       " 'from',\n",
       " 'the',\n",
       " 'store.Should',\n",
       " 'I',\n",
       " 'pick',\n",
       " 'up',\n",
       " 'some',\n",
       " 'black-eyed',\n",
       " 'peas',\n",
       " 'as',\n",
       " 'well?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# white spaces: \\s+\n",
    "\n",
    "whitespace_tokenizer=RegexpTokenizer(\"\\s+\",gaps=True)\n",
    "print(whitespace_tokenizer)\n",
    "whitespace_tokenizer.tokenize(Text)\n",
    "\n",
    "# the result is exactly same as we had done through Text.split(' ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'Mr', 'Smith', 'Should']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find words starting with capital letters: [A-Z][\\w]+\n",
    "\n",
    "capital_tokenizer=RegexpTokenizer('[A-Z][\\w]+')\n",
    "capital_tokenizer.tokenize(Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Remove: capital letters, punctuation, numbers, stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Mr  Smith  I’m going to buy some vegetables  tomatoesand cucumbers  from the store Should I pick up some black eyed peas as well '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # Regular expression library\n",
    "import string\n",
    "\n",
    "# Replace punctuations with a white space\n",
    "\n",
    "clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ',Text)\n",
    "clean_text\n",
    "\n",
    "# see the output I'm remain I'm but black-eyed becomes black eyed,well? becomes well,Mr. becomes Mr,Smith! becomes Smith\n",
    "# ( , ) ,. are also removed etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi mr  smith  i’m going to buy some vegetables  tomatoesand cucumbers  from the store should i pick up some black eyed peas as well '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text=clean_text.lower()\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi mr  smith  i’m going to buy some vegetables  tomatoesand cucumbers  from the store should i pick up some black eyed peas as well '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes all words containing digits\n",
    "clean_text = re.sub('\\w*\\d\\w*', ' ', clean_text)\n",
    "clean_text\n",
    "\n",
    "# there is no number so it return same text which we have given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Tricks: Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "def square_me(x):\n",
    "    return x*x\n",
    "\n",
    "print(square_me(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "# writing above function using lamba techniq\n",
    "\n",
    "square_me_too=lambda x: x*x\n",
    "\n",
    "print(square_me_too(9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Tricks: Lambdas and Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 25, 49, 121]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers=[1,5,7,11]\n",
    "\n",
    "final_value=list(map(square_me_too,numbers))\n",
    "final_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm going to buy   cans of beans\",\n",
       " \"I'm going to   of hams\",\n",
       " \"I'm going to   Dore Street\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaning bunch of texts\n",
    "\n",
    "text1=\"I'm going to buy 5 cans of beans\"\n",
    "text2=\"I'm going to 6lbs of hams\"\n",
    "text3=\"I'm going to 111 Dore Street\"\n",
    "\n",
    "text=[text1,text2,text3]\n",
    "\n",
    "remove_numbers=lambda x: re.sub('\\w*\\d\\w*',' ',x)\n",
    "\n",
    "result=list(map(remove_numbers,text))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Stop Words\n",
    "Stop words are words that have very little semantic value.\n",
    "There are language and context-specific stop word lists online that you can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "a=set(stopwords.words('english'))\n",
    "print('--------------------------------')\n",
    "print(len(a))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'as', 'black', 'buy', 'cucumbers', 'eyed', 'from', 'going', 'hi', 'mr', 'peas', 'pick', 'should', 'smith', 'some', 'store', 'the', 'to', 'tomatoes', 'up', 'vegetables', 'well']\n",
      "22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>as</th>\n",
       "      <th>black</th>\n",
       "      <th>buy</th>\n",
       "      <th>cucumbers</th>\n",
       "      <th>eyed</th>\n",
       "      <th>from</th>\n",
       "      <th>going</th>\n",
       "      <th>hi</th>\n",
       "      <th>mr</th>\n",
       "      <th>...</th>\n",
       "      <th>should</th>\n",
       "      <th>smith</th>\n",
       "      <th>some</th>\n",
       "      <th>store</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>tomatoes</th>\n",
       "      <th>up</th>\n",
       "      <th>vegetables</th>\n",
       "      <th>well</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  as  black  buy  cucumbers  eyed  from  going  hi  mr  ...   should  \\\n",
       "0    1   1      1    1          1     1     1      1   1   1  ...        1   \n",
       "\n",
       "   smith  some  store  the  to  tomatoes  up  vegetables  well  \n",
       "0      1     2      1    1   1         1   1           1     1  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writing in two line but it is a single sentence\n",
    "my_text = [\"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers)\"\\\n",
    "\"from the store. Should I pick up some black-eyed peas as well?\"]\n",
    "\n",
    "# Incorporate stop words when creating the count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# how many times each word is coming in given document\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(my_text)\n",
    "print(cv.get_feature_names())\n",
    "print(len(cv.get_feature_names()))\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['black', 'buy', 'cucumbers', 'eyed', 'going', 'hi', 'mr', 'peas', 'pick', 'smith', 'store', 'tomatoes', 'vegetables']\n",
      "13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>black</th>\n",
       "      <th>buy</th>\n",
       "      <th>cucumbers</th>\n",
       "      <th>eyed</th>\n",
       "      <th>going</th>\n",
       "      <th>hi</th>\n",
       "      <th>mr</th>\n",
       "      <th>peas</th>\n",
       "      <th>pick</th>\n",
       "      <th>smith</th>\n",
       "      <th>store</th>\n",
       "      <th>tomatoes</th>\n",
       "      <th>vegetables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   black  buy  cucumbers  eyed  going  hi  mr  peas  pick  smith  store  \\\n",
       "0      1    1          1     1      1   1   1     1     1      1      1   \n",
       "\n",
       "   tomatoes  vegetables  \n",
       "0         1           1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare it with above one\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "X = cv.fit_transform(my_text)\n",
    "print(cv.get_feature_names())\n",
    "print(len(cv.get_feature_names()))\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Stemming & Lemmatization = Cut word down to base form\n",
    "• Stemming: Uses rough heuristics to reduce words to base<br/>\n",
    "• Lemmatization: Uses vocabulary and morphological analysis\n",
    "    \n",
    "### Multiple stemmers available in NLTK\n",
    "• PorterStemmer, LancasterStemmer, SnowballStemmer<br/>\n",
    "• WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive: driv\n",
      "drives: driv\n",
      "driver: driv\n",
      "drivers: driv\n",
      "driven: driv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer=LancasterStemmer()\n",
    "\n",
    "print(\"drive: {}\".format(stemmer.stem('drive')))\n",
    "print(\"drives: {}\".format(stemmer.stem('drives')))\n",
    "print(\"driver: {}\".format(stemmer.stem('driver')))\n",
    "print(\"drivers: {}\".format(stemmer.stem('drivers')))\n",
    "print(\"driven: {}\".format(stemmer.stem('driven')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive: drive\n",
      "drives: drive\n",
      "driver: driver\n",
      "drivers: driver\n",
      "driven: driven\n"
     ]
    }
   ],
   "source": [
    "# see difference from above one and below one\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer1=PorterStemmer()\n",
    "\n",
    "print(\"drive: {}\".format(stemmer1.stem('drive')))\n",
    "print(\"drives: {}\".format(stemmer1.stem('drives')))\n",
    "print(\"driver: {}\".format(stemmer1.stem('driver')))\n",
    "print(\"drivers: {}\".format(stemmer1.stem('drivers')))\n",
    "print(\"driven: {}\".format(stemmer1.stem('driven')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive: drive\n",
      "drives: driv\n",
      "driver: driv\n",
      "drivers: driv\n",
      "driven: driv\n"
     ]
    }
   ],
   "source": [
    "# see difference from above two\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer2=SnowballStemmer('english')\n",
    "\n",
    "print(\"drive: {}\".format(stemmer2.stem('drive')))\n",
    "print(\"drives: {}\".format(stemmer.stem('drives')))\n",
    "print(\"driver: {}\".format(stemmer.stem('driver')))\n",
    "print(\"drivers: {}\".format(stemmer.stem('drivers')))\n",
    "print(\"driven: {}\".format(stemmer.stem('driven')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive: drive\n",
      "drives: drive\n",
      "driver: driver\n",
      "drivers: driver\n",
      "driven: driven\n"
     ]
    }
   ],
   "source": [
    "# out of these four i prefer this one \n",
    "\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "print(\"drive: {}\".format(lemmatizer.lemmatize('drive')))\n",
    "print(\"drives: {}\".format(lemmatizer.lemmatize('drives')))\n",
    "print(\"driver: {}\".format(lemmatizer.lemmatize('driver')))\n",
    "print(\"drivers: {}\".format(lemmatizer.lemmatize('drivers')))\n",
    "print(\"driven: {}\".format(lemmatizer.lemmatize('driven')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now open the link and read it\n",
    "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Parts of Speech Tagging\n",
    "\n",
    "• Parts of speech tagging labels each word as a part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', 'NNP'),\n",
       " ('Smith', 'NNP'),\n",
       " ('lives', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('United', 'NNP'),\n",
       " ('States', 'NNPS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "my_text = \"James Smith lives in the United States.\"\n",
    "\n",
    "pos=pos_tag(word_tokenize(my_text))\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "---------------------\n",
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('tagsets')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "nltk.help.upenn_tagset()\n",
    "\n",
    "# it gives output that which symbol reprsents which parts of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) aka Entity Extraction<br/>\n",
    "• Identifies and tags named entities in text (people, places, organizations, phone\n",
    "numbers, emails, etc.)<br/>\n",
    "• Can be tremendously valuable for further NLP tasks<br/>\n",
    "• For example: “United States” --> “United_States”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "\n",
    "\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "my_text='James Smith lives in the United States.'\n",
    "\n",
    "# this labels each word as a part of speech\n",
    "tokens=pos_tag(word_tokenize(my_text))\n",
    "\n",
    "# this extracts entities from the list of words\n",
    "entities=ne_chunk(tokens)\n",
    "\n",
    "entities.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Compound Term Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting and tagging compound words or phrases in text<br/>\n",
    "• This can be very valuable for special cases<br/>\n",
    "• For example: “black eyed peas“ --> “black_eyed_peas”<br/>\n",
    "• This totally changes the conceptual meaning!<br/>\n",
    "• Named entity recognition groups together words and identifies entities, but doesn’t \n",
    "capture them all, so you can identify your own compound words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You_all', 'are', 'the', 'greatest', 'students', 'of_all_time', '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multi-word expression\n",
    "\n",
    "from nltk.tokenize import MWETokenizer \n",
    "\n",
    "my_text = \"You all are the greatest students of all time.\"\n",
    "\n",
    "mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
    "\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
    "mwe_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
